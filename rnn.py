# -*- coding: utf-8 -*-
"""RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5W2T4FJndjYMFkBUk2_tsddJlhuHSRv
"""

# import dependencies

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from keras.utils import np_utils

from google.colab import drive
drive.mount('/content/drive')

# TODO

# load Shakespeare sonnets
# text from Project Gutenberg, modified to remove credits
# http://www.gutenberg.org/cache/epub/1041/pg1041.txt

# TODO

text = (open("/content/drive/My Drive/Colab Notebooks/shakespeare_sonnets.txt").read().lower())
print("Length of corpus: {0} characters.".format(len(text)))

# map characters to index values

characters = sorted(list(set(text)))
n_to_char = {n:char for n, char in enumerate(characters)}
char_to_n = {char:n for n, char in enumerate(characters)}
print("Created indices for {0} characters.".format(len(characters)))
print(characters)

# create training sequences
# each one is 100 characters long
# the "label" is the next character in Shakespeare's work
X = [ ]
Y = [ ]
total_length = len(text)
SEQUENCE_LENGTH = 100
for i in range(0,total_length-SEQUENCE_LENGTH,1):
  sequence = text[i:i+SEQUENCE_LENGTH]
  label = text[i+SEQUENCE_LENGTH]
  X.append([char_to_n[c] for c in sequence])
  Y.append(char_to_n[label])
print("Generated {0} sequences.".format(len(X)))
print(X[0])

# TODO

# reshape training data
# also make labels categorical

X_adjusted = np.reshape(X,(len(X),SEQUENCE_LENGTH,1))
X_adjusted = X_adjusted / float(len(characters))
Y_adjusted = np_utils.to_categorical(Y)

# define model
model = Sequential()

model.add(LSTM(400, input_shape=(X_adjusted.shape[1], X_adjusted.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(400))
model.add(Dropout(0.2))
model.add(Dense(38, activation='softmax'))
  
# TODO

model.summary()

# compile model

model.compile(loss='categorical_crossentropy', optimizer='adam')

# fit model and save weights
# with gpu, 512 batch size, 0.1 val split: about 3 minutes/epoch

history = model.fit(X_adjusted, Y_adjusted, batch_size=512, validation_split=0.1, epochs=2)

# TODO

model.save("/content/drive/My Drive/Colab Notebooks/shakespeare_weights_10_epochs.h5")

# generate some text using the trained model
# generating 256 characters takes about 1 minute

def generate_text(start_index,chars_to_generate):
  
  # choose which 100-character example to start from
  recent_chars_by_index = X[start_index][0:SEQUENCE_LENGTH]

  # convert example from index values to characters
  # (first 100 characters of 'generated string' will be this starting example)
  generated_characters = [n_to_char[n] for n in recent_chars_by_index]

  # generate additional characters
  for i in range(chars_to_generate):
  
    # reshape and normalize to match model input
    #x = np.reshape(recent_chars_by_index,(1,len(recent_chars_by_index), 1))
    x = np.reshape(recent_chars_by_index,(1,SEQUENCE_LENGTH, 1))
    x = x / float(len(characters))
    
    # predict next character
    predicted_index = np.argmax(model.predict(x, verbose=0))
    generated_characters.append(n_to_char[predicted_index])

    # update recent chars
    recent_chars_by_index.append(predicted_index)
    recent_chars_by_index = recent_chars_by_index[-SEQUENCE_LENGTH:]

  return ''.join(generated_characters)
  
# print generated string
print(generate_text(999,256))

# load a (very) pre-trained model and generate text with it
# note that model architecture is the same as what we are currently using
# (or else it wouldn't work!)

model.load_weights("/content/drive/My Drive/Colab Notebooks/text_generator_400_0.2_400_0.2_100.h5")
print(generate_text(999,256))